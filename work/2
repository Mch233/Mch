聚类算法

引言
	目的：

		根据元素的相似性将元素分类

	应用范围
		天文学
		生物信息学
		文献计量学
		模式识别

	算法根基

		聚类中心的密度高于其邻居，而密度高的点相对较远

	算法特点
		簇的数量直观地产生
		异常值被自动地发现并从分析中排除
		无论它们的形状和它们所嵌入的空间的维度如何，都能被识别出来

段落１背景
	聚类算法尝试根据元素的相似性将元素分类

	目前已经提出了几种不同的聚类策略（1），但即使在聚类的定义上也没有达成共识

	对于K-means和K-medoids方法
		聚类:  距，聚类中心很小的距离为特征的数据组 
		目标函数: 与一组假定的聚类中心的距离之和

		缺陷：
			数据点始终分配给最近的中心－－>无法检测到非球形团簇（7）

		改进
		基于分布的算法，
		将数据点实现，作为预定义概率分布函数的混合; 
		缺点：这种方法的准确性取决于试验概率代表数据的能力

段落２现有的解决措施
	方案１
	策略：通过基于数据点局部密度的方法检测具有任意形状的簇
	　　　在具有噪声的应用的、基于密度的空间聚类中：
			选择密度阈值
				将密度低于该阈值的区域中的点丢弃为噪声
				并将高密度的不连续区域分配给不同的簇
	缺陷：选择适当的阈值不是很容易；这是平均移位聚类方法中不存在的缺点

	方案２
	策略：集群定义为一组收敛于密度分布函数的相同局部最大值的点；这种方法可以发现非球形团簇
	缺陷：仅适用于由一组坐标定义的数据，并且计算成本很高。

段落３改进－提出了一种新方法－特点
		１．仅基于数据点之间的距离							＃类似K-medoids方法
		２．可以检测非球形集群并自动找到集群的正确数量　　	＃类似DBSCAN和均值漂移方法
		３．聚类中心：数据点密度中的局部最大值　　　　　　　　　　＃类似mean－shift方法
		４．不需要将数据嵌入向量空间中，并且能明确地将每个数据点的密度字段最大化　

段落４算法的基础假设、对于每个数据点i,计算两个量

	聚类中心被局部密度较低的邻居包围，并且与任何具有较高局部密度的点处于相对较大的距离

	对于每个数据点i,计算两个量：
		１．局部密度ri
		２．距密度较高点的距离di
		说明：
		这两个量仅取决于数据点之间的距离dij，假定它们满足三角不等式。
		 
		1.数据点i的局部密度ri被定义为

			公式：

			其中如果x <0，则x（x）= 1，否则x（x）= 0，并且dc是截止距离

			基本上，ri等于比dc更接近点i的点数

			该算法仅对ri在不同点的相对大小敏感，这意味着，对于大数据集，分析结果对于dc的选择是稳健的。
	 
		2.di是通过计算点i和任何其他密度较高的点之间的最小距离来测量的：

			公式：

段落５聚类中心
	对于密度最高的点，通常采用di = 公式

	请注意，di比典型的最近邻距离要大得多
	仅适用于密度为局部或全局最大值的点。
	因此，聚类中心被认为是di的值异常大的点。

段落６举例说明　图１－－找聚类中心

	图1A
	嵌入在二维空间中的28个点。 
	密度最大值在点1和10，将其确定为聚类中心。

	图1B
	每个点的di作为ri函数的图; 
	（A）中数据的决策图。 不同的颜色对应于不同的群集。
	横坐标－局部密度r，纵坐标－距密度较高点的距离d

		对于点9和点10
		具有相似的局部密度r
		但是距密度较高点的距离d差异很大

		点9属于点1的集群，并且具有更高r的其他点非常接近它，
		而最接近的更高点 点10的密度属于另一个集群。

	因此，正如预期的那样:
		高d和高r的唯一点是聚类中心
	
	具有相对较高的d和较低的r点26,27和28,被认为是由单一点组成的集群，即异常值

	#疑问? 图B确定聚类中心，怎么分类的？

段落７聚
	在找到聚类中心之后
	每个剩余点将被分配到与其最近邻居密度相同的同一个聚类
	优点：与其他聚类算法（其中目标函数被迭代地优化）相比，聚类分配在单个步骤中执行

段落８衡量算法可靠性
	基于函数优化的方法
		度量：其收敛值
	类似DBSCAN的方法中
		度量：密度值高于阈值的可靠点（可能导致低密度簇，如图1中的被归类为噪音。） 

	本算法
		不引入噪声信号截止点
		相反的
		１．为每个群集找到一个边界区域（分配给该群集的点集，距离属于其他群集的数据点的距离为dc）
		２．为每个群集找出其边界区域内密度最高的点（用rb来表示它的密度）
		密度高于rb的集群点认为是集群核心的一部分（鲁棒分配），其他视为群晕的一部分（噪音）

段落９基准测试　图２

	图2A
		数据点是从具有非球形和强重叠峰的概率分布绘制的;
		对应的最大值的概率值相差几乎一个数量级

		以图2A中的分布画出
			Ｂ4000点　决策图Ｄ
			Ｃ1000点　决策图Ｅ

	从决策图中观察到d值大且密度相当大的五个点（聚类中心）在图中以大实心圆表示
	选择了中心之后，每个点都被分配到一个簇或者晕圈。

	实验表明：
	１．该算法捕捉概率峰值的位置和形状，甚至包含非常不同的密度（图2C中的蓝色和浅绿色点）和非球形峰值

	２．通过目视检查图2A中的概率分布，分配给晕圈的点不会被分配给任何峰值的区域

段落１０定量地证明鲁棒性

	从图2A中的分布中绘制10,000个点来进行分析，以该样本上获得的聚类分配为参考
	仅保留一小部分点来获得缩减的样本；独立地为每个缩减的样本执行聚类分配

	图2F
	as a function of the size of the reduced sample, the fraction of points assigned to a cluster different than the one they were assigned to in the reference case. 
	作为减少样本的大小的函数
	分配给一个簇的点的分数　与　在参考用例中分配给它们的分数不同　　　？？？
	
	实验结果：
		对于包含1000个点的小样本，错误分类的百分比仍然远远低于1%。
	
段落１１存在的缺陷
	不同dc的数据在图2B中产生了一致的结果(图S1)

	根据经验选择dc
		以使相邻的平均数量约为数据集中总点数的1至2％。

	缺点：
		对于由少量点组成的数据集
			ri可能会受到大量统计错误的影响。
			此时，用更精确的方法估计密度可能有用

段落１２基准测试

	对图3所示的测试用例进行基准测试

	为了计算少数点情况下的密度，
	实验采用了（11）中描述的指数核

	图3A
	数据集：应用（12）数据集
	本算法：获得的结果与原始文章的结果相当
	其他常用算法：失败

	图3B
	数据集：从（13）中取数据分布中具有高重叠的15个簇
	本算法：成功地确定了数据集的簇结构

	图3C
	火焰方法(由局部近似的隶属度的模糊聚类)的测试用例(14)
	其结果与原始方法相当

	图4D
	在最初为了说明图4D所示的基于路径的谱聚类（15）的性能而引入的数据集中
	本算法正确地找到三个聚类，而不需要生成连接图
	

	对比实验
		S3和S4为基于同样的数据集，用K-means(2)所获得的聚类
		对比结果
		即使K-means值使用正确的K值进行优化，在大多数情况下，赋值不符合视觉直觉

段落１３
	对于不显著影响到dc以下距离的度规的变化具有鲁棒性，即保持Eq. 1中的密度估计量不变
	显然,Eq.2中的距离会影响这种变化的指标	
	但容易认识到，决策图的结构（特别是具有大的d值的数据点的数量）是密度值排序的结果 而不是远距离点之间的实际距离。 
	证明这种说法的例子如图3所示。

段落１４

	本算法只需要测量所有成对数据点之间的距离
	而不需要参数化概率分布（8）或多维密度函数（10）
	因此，其性能不受嵌入数据点的空间的固有维度的影响

	实验验证
		在一个包含16个256维（16）簇的测试用例中
		该算法找到了簇的数目并正确分配了这些点（图S6）

		对三种类型小麦种子的七个x射线特征的210个测量值的数据集
		该算法正确地预测了三种类型的小麦的存在
		正确地将97%的点分配给聚类中心

段落１５Olivetti Face聚类
	Olivetti Face数据库(18)
		挑战
		对密度的可靠估计变得困难
		因为“理想”数量的聚类与数据集中的元素数量相当

	两个图像之间的相似度由以(19)计算。
	密度通过高斯核（11）估计，方差dc = 0:07

	对于这样一个小的集合
	密度估计器不可避免地受到较大的统计误差的影响
	因此，将图像分配给一个比前面的示例稍微更严格的标准

		只有当图像的距离小于dc时，图像才会被分配到与更高密度的最近图像的同一簇。
		因此，图像比dc的任何其他图像都要高，密度仍未赋值

	图4为对数据集中前100个图像执行的分析结果

	图4A决策图
	显示了几个不同密度最大值的存在。
	由于数据点的稀疏性
	与其他例子不同的是，它们的确切数字并不清楚

	图4B
	提示：选择中心数量由按降序排列的gi-ridi图提供
	这张图显示，这个数量，根据定义，对于集群中心来说是很大的
	低于一个等级9开始出现异常
	因此，我们用9个中心进行了分析

	图4D
	用不同的颜色显示与这些中心相对应的簇
	七个集群对应不同的主题，表明该算法能够“识别”10个中的7个主题
	第八个主题出现在两个不同的集群中
	
	图4C
	对数据库的所有400幅图像执行分析，决策图并不能清楚地识别簇的数量（图S9）
	但是，在图4C中表明通过增加越来越多的假定中心，可以明确地识别大约30个主题（图S9）

		当包含更多的中心时，一些主题的图像被分割在两个集群中
		但所有的集群仍然是纯的，即只包含同一主题的图像

		在（20）之后
		我们还计算了
		与同一个聚类（rtrue）正确关联的同一主题的图像对的比例
		以及错误分配给同一个聚类（rfalse）的不同主题的图像对的比例

		如果聚类分配中没有在dc分量上应用截止值（即如果在我们的一般公式中应用了我们的算法）
		那么得到〜68％和rfalse〜1.2％，约42〜50个中心
		可与最先进的用于无监督图像分类的方法相媲美

段落１６分子动力学轨迹聚类
	最后
	将聚类算法用于分析三聚赖氨酸在300 K水中的分子动力学轨迹（21）

	在这种情况下，团簇将大致对应动能盆地，即系统的独立构象
	其在相当长的时间内稳定并被自由能垒隔开，在微观时间尺度上很少被穿过

	 通过标准方法（22）基于动力学矩阵的谱分析来分析轨迹
	 其动力学矩阵的特征值与系统的弛豫时间相关联
	 在第七个特征值之后存在间隙（图S10），表明系统有八个盆地
	 与此相一致，本算法的聚类分析（图S10）产生了8个聚类
	 它的构造与定义动力学盆地的构造一一对应

段落１７

	和其他基于密度的聚类算法中使用密度极大值来识别集群，是一种简单且直观的选择
	但它有一个重要的缺点:
		如果一个人随机生成数据点，那么对有限样本的密度估计是远远不一致的，而是以几个极大值为特征
	然而
	决策图使我们能够区分真正的簇和由噪声产生的密度波纹
	定性地说
	只有在前一种情况下，对应于聚类中心的点与r和d中的其他点相距较远。
	对于一个随机分布，而是观察到一个连续的分布的r和d值

	实际上
	我们对在超立方体中从均匀分布中随机生成的点集进行了分析
	公式中
	用超立方体上的周期边界条件计算数据点之间的距离1和2

	这一分析表明
		对于随机分布的数据点
		数量gi = ridi根据幂律分布
		指数取决于嵌入点的空间的维度

		数据集的g的分布与真实的簇类，如图2到４中所示

		与幂律法有明显的不同，特别是高g区域(图11)

		该观察结果
		也许
		可为聚类中心的自动选提供一个标准、根基
		也许
		从统计学上验证了用我们的方法进行分析的可靠性