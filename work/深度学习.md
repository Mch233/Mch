#深度学习
自动地学习特征的方法，统称为Deep Learning
“深度模型”是手段，“特征学习”是目的
通过逐层特征变换，将样本在原空间的特征表示变换到一个新特征空间，
从而使分类或预测更加容易。

传统神经网络中，采用的是back propagation的方式进行，简单来讲就是采用迭代的算法来训练整个网络，随机设定初值，计算当前网络的输出，然后根据当前输出和label之间的差去改变前面各层的参数，直到收敛（整体是一个梯度下降法）。

deep learning整体上是一个layer-wise的训练机制

1. 使用自下上升非监督学习（就是从底层开始，一层一层的往顶层训练）：

       采用无标定数据（有标定数据也可）分层训练各层参数，这一步可以看作是一个无监督训练过程，是和传统神经网络区别最大的部分（这个过程可以看作是feature learning过程）：

       具体的，先用无标定数据训练第一层，训练时先学习第一层的参数（这一层可以看作是得到一个使得输出和输入差别最小的三层神经网络的隐层），由于模型capacity的限制以及稀疏性约束，使得得到的模型能够学习到数据本身的结构，从而得到比输入更具有表示能力的特征；在学习得到第n-1层后，将n-1层的输出作为第n层的输入，训练第n层，由此分别得到各层的参数；

2. 自顶向下的监督学习（就是通过带标签的数据去训练，误差自顶向下传输，对网络进行微调）：

       基于第一步得到的各层参数进一步fine-tune整个多层模型的参数，这一步是一个有监督训练过程；第一步类似神经网络的随机初始化初值过程，由于DL的第一步不是随机初始化，而是通过学习输入数据的结构得到的，因而这个初值更接近全局最优，从而能够取得更好的效果；所以deep learning效果好很大程度上归功于第一步的feature learning过程

#自编码神经网络

三层BP网络，输出等于输入
其输出是对输入的一种重构

中间层，可以看作是对原始输入数据的某种特征表示。
第三层去掉，这样变成一个两层的网络。
把这个学习到特征再用同样的方法创建一个三层BP网络，
第二次创建的三层网络的输入是上一个网络的中间层的输出
用同样的训练算法，对第二个自联想网络进行学习。
那么，第二个自联想网络的中间层是对其输入的某种特征表示。
按照这种方法，依次创建很多这样的由自联想网络组成的网络结构，
这就是深度神经网络

深度神经网络在每一层是对最原始输入数据在不同概念的粒度表示，也就是不同级别的特征描述。

##与PCA的联系

网络中不使用sigmoid函数，而使用线性函数，就是PCA模型。
中间网络节点个数就是PCA模型中的主分量个数。
不用担心学习算法会收敛到局部最优，因为线性BP网络有唯一的极小值。
