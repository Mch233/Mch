https://blog.csdn.net/qq_37655759/article/details/57445246

Why Does Unsupervised Pre-training Help Deep Learning?

无监督预训练可以增加深层网络的鲁棒性
增加无预训练结构的深度，会增加找到差的明显局部最小值的可能性
预训练网络可以始终有更好的泛化
预训练网络比没有预训练网络可以定性地学习到不同的特征

无监督预训练不单单只是一种能得到好的初始化边缘分布的方法，它还能捕获参数间复杂的依赖关系。

无监督预训练的深层网络似乎表现出一些正则化的特性：对于足够小的层数，预训练深层结构比随机初始化的深层结构更差。而且，当层数足够大，预训练模型的训练误差大，但是泛化性能好。


本文考虑了两种无监督预训练模型—降噪自动编码器和受限玻尔兹曼机，两种模型有相似的结果。
即使在非常大的训练集中预训练的优势依旧存在，指出结论：非凸优化问题的开始点真的很重要
无监督预训练作为方差减少技术，但是在大数据集中预训练网络有较低训练误差，这一结果支持优化解释

结构的动态学习分为两阶段，无监督预训练和有监督的微调
监督目标函数的非线性有很多影响。其中的一个影响是早期的样例对训练输出有很大的影响，这也是为什么在大规模数据集下无监督预训练的影响依然存在。

无监督预训练作为正则化矩阵，只影响监督训练的开始点，与典型正则化不同的是，数据增加正则化作用不消失。

训练误差和测试误差经过预训练都会减少

深层网络的优化是一个复杂的问题，受训练中早期样例的严重影响，未来的工作应该证明这个假设。如果这是真的，我们希望学者捕获真实复杂的分布，这可能意味着我们应该考虑学习算法来减少早期样例的影响

除了上述的假设，未来的工作应该调查本文结果与Hinton and Salakhutdinov (2006)结果之间的关联，Hinton表示在深度自编码器中，很难得到好的训练重构误差。未来工作还可以包括分析和理解深度半监督技术，即预训练过程和监督过程不分离。我们期望尽管分析结果和我们的类似，但是可能会暴露出一些问题。
