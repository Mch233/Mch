1. 扩展到连续输入

一种直观的方法，是将输入的实属向量进行归一化， 转成(0, 1)区间的数。然后用正常的RBM的CD-k来训练就行。
将它们缩放到（0,1）区间并将每个输入连续值视为二进制随机变量取值为1的概率。
先对数据进行归一化，然后使用二值可见单元的实值概率取代激活情况。
于实值-实值的情况以后再讨论，其实有人说，实值-实值的RBM已经退化成PCA了，后续继续研究。

作者从RBM的能量函数入手，将输入转成高斯unit，然后用CD-k算法就可以训练。具

2. 将隐含层扩展成连续值的形式

上述方法也可以用到隐含层。

3. Understanding why the layer-wise strategy works

作者用autoencoder来替换DBN中的RBM，得到了comparable的实验结果。作者用surperwised训练算法来代替RBM的unsurpervised训练算法，发现结果略差，作者的解释是：surperwised的方法过于“贪心”，在训练过程中丢掉了部分信息。

对DBN层次式培训战略的明显成功的合理解释是这一点
无监督的预训练有助于更好地缓解深层网络的困难优化问题
初始化所有图层的权重。 

无监督学习方法的主要目的是提取一般有用的
来自未标记数据的特征，检测和消除输入冗余，以及
只保留强健和歧视性表示中的数据的重要方面。

