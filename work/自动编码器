#浅层自编码器

至少有一层隐藏层且隐藏单元足够多的前馈网络能以任意精度近似任意函数
即具有单隐藏层的自编码器在数据域内能表示任意近似数据的恒等函数

模型
	X   ---f---> h ---g---> R
	避免使自动编码器输入到输出完全相等　//数据优先复制
	防止模型仅仅学习一个恒等函数

用途
	降维
	特征学习
	生成式建模	//自编码器＋潜变量模型理论

前馈网络的特例

训练方法：
	小批量梯度下降法
	再循环

１．欠完备自动编码器
	h < x 		//隐含层维度小于输入层维度
	缺陷：
	如果自编码器的容量太大，就无法学习到数据集的任何有用信息
	当h > x or h = x 时会有类似缺陷

２．正则编码器

	不一样的损失函数学习其他特性：稀疏表示、表示小的导数、对噪声和输入缺失的鲁棒性

３．稀疏自编码器×

	在哪？添加稀疏惩罚

４．去噪自编码器
	
	ｘ加入噪声

５．收缩自编码器
	编码时惩罚导数作为正则
	学习可以反映训练数据分部信息的特征

６．随机编码器和解码器

	应用潜变量模型作为编码/解码函数


